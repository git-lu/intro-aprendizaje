{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../images/ods_stickers.jpg\" />\n",
    "    \n",
    "# Introducción al Machine Learning 2020\n",
    "\n",
    "Basado en material de Pedro Pury y  Sebastian Raschka (sraschka@wisc.edu) Traducido y editado al español por [Ana Georgina Flesia](https://www.linkedin.com/in/georginaflesia/). Este material esta sujeto a los términos y condiciones de la licencia  [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Se permite el uso irrestricto para todo propósito no comercial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1: Arboles de decision\n",
    "\n",
    "Al entrar al bar, sus cinco amigos, quienes se reúnen al salir\n",
    "de trabajar, ya tiene sus bebidas sobre la mesa.\n",
    "María que tiene un cargo de gerente paga la ronda la mitad de las\n",
    "veces. Pablo paga la ronda la cuarta parte de las veces, mientras\n",
    "que Sara y Carlos que son becarios pagan indistintamente entre ambos\n",
    "la octava parte de las veces. Juan nunca sacó la billetera desde\n",
    "que se reúnen los viernes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)  ¿Qué fracción de las veces ud.\\ paga la ronda?\n",
    "\n",
    "Aparentemente nunca pago la ronda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/2 + 1/4 + 1/8 + 1/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calcular la entropía de la distribución de probabilidad\n",
    "con la que cada uno paga la ronda. ¿Cuál es el número medio\n",
    "de preguntas (de respuesta binaria) que necesitan hacerse para\n",
    "saber quién paga la ronda?  \n",
    "\n",
    "P: probabilidad de que la persona pague la ronda  \n",
    "- P = 1/2 => la entropia es 0 pues María es el unico elemento del conjunto  \n",
    "- P = 1/4 => la entropia es 0 tambien pues Pablo es el unico elemento del conjunto\n",
    "- P = 1/8 => la entropia es 1 pues si tomamos a cada persona como una clase, tenemos la misma proporcion de cada una de las clases\n",
    "- P = 0 => la entropia es 1 tambien pues estamos Juan y yo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) \n",
    "Poco después de arribar al bar se suman dos antiguos amigos quienes\n",
    "no vivieron en la ciudad el último año. Ellos deciden que la próxima\n",
    "ronda la debe pagar ud.\\ y conociendo que cursa esta materia,\n",
    "lo desafían a predecir que bebida tomará cada uno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) \n",
    "Usando la información de las tres variables binarias:\n",
    "sexo, si es o no estudiante y si le gusta o no bailar,\n",
    "y recordando la elección de bebidas de la noche anterior,\n",
    "puede construirse la siguiente tabla:\n",
    "\n",
    "\n",
    "    |Bebida | Género | Estudiante | Baile |\n",
    "    ---------------------------------------------\n",
    "    cerveza | M      | T          | T  \n",
    "    cerveza | M      | F          | T  \n",
    "    vodka   | M      | F          | F  \n",
    "    vodka   | M      | F          | F  \n",
    "    vodka   | F      | T          | T  \n",
    "    vodka   | F      | F          | F  \n",
    "    vodka   | F      | T          | T \n",
    "    vodka   | F      | T          | T \n",
    "    -------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "e) Usando entropía de información entrenar un árbol\n",
    "de decisión a partir de la tabla anterior.\n",
    "Registrar todos los valores calculados para elegir las variables\n",
    "en cada nodo del árbol.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ganancia de información que obtengo partiendo por genero es: 0.5\n",
      "La ganancia de información que obtengo partiendo por estudiante es: 0.23648016060101568\n",
      "La ganancia de información que obtengo partiendo por baile es: 0.3931558784658321\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "# La entropia si elijo cada una de las caracteristicas como raiz del arbol\n",
    "entropia_genero_M =  - (1/2) * log((1/2),2) -  (1/2) * log((1/2),2)\n",
    "entropia_genero_F =  - 1 * log(1,2) \n",
    "gain_genero = 1 - (4/8) * entropia_genero_M - (4/8) * entropia_genero_F\n",
    "print(\"La ganancia de información que obtengo partiendo por genero es: {}\".format(gain_genero))\n",
    "\n",
    "entropia_estudiante_T = - (1/4) * log(1/4,2) - (3/4) * log(3/4)\n",
    "entropia_estudiante_F = - (1/4) * log(1/4,2) - (3/4) * log(3/4,2)\n",
    "gain_estudiante = 1 - (4/8) * entropia_estudiante_T - (4/8) * entropia_estudiante_F\n",
    "print(\"La ganancia de información que obtengo partiendo por estudiante es: {}\".format(gain_estudiante))\n",
    "\n",
    "entropia_baile_T = - (2/5) * log((2/5),2) - (3/5) * log((3/5),2) \n",
    "entropia_baile_F = - (3/3) * log((3/3),2)\n",
    "gain_baile = 1 - (5/8) * entropia_baile_T - (3/8) * entropia_baile_F\n",
    "print(\"La ganancia de información que obtengo partiendo por baile es: {}\".format(gain_baile))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a partir por genero y analizar de nuevo la ganancia de información por entropia.  \n",
    "Como ya tenemos un conjunto puro en el genero femenino, no vamos a profundizar el analisis ahi  \n",
    "Ahora tenemos dos conjuntos con 4 elementos cada uno. y vamos analizar la ganancia de información partiendo por otra caracteristica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La ganancia de información que obtengo partiendo por genero y estudiante es: 0.31127812445913283\n",
      "La ganancia de información que obtengo partiendo por genero y baile es: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "entropia_genero_M_estudiante_T =  - (1) * log((1),2)\n",
    "entropia_genero_M_estudiante_F =  - (1/3) * log((1/3),2) -  (2/3) * log((2/3),2)\n",
    "gain_genero_estudiante = 1 - (1/4) * entropia_genero_M_estudiante_T - (3/4) * entropia_genero_M_estudiante_F\n",
    "print(\"La ganancia de información que obtengo partiendo por genero y estudiante es: {}\".format(gain_genero_estudiante))\n",
    "\n",
    "entropia_genero_M_baile_T =  - (2/2) * log((2/2),2)\n",
    "entropia_genero_M_baile_F =  - (2/2) * log((2/2),2)\n",
    "gain_genero_baile = 1 - (2/4) * entropia_genero_M_baile_T - (2/4) * entropia_genero_M_baile_F\n",
    "print(\"La ganancia de información que obtengo partiendo por genero y baile es: {}\".format(gain_genero_baile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es claro que ahora partimos por baile y obtenemos dos conjuntos puros. El arbol tiene la siguiente pinta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"218pt\" height=\"218pt\"\n",
       " viewBox=\"0.00 0.00 217.70 218.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 214)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-214 213.6967,-214 213.6967,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"121.6967\" cy=\"-192\" rx=\"34.394\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"121.6967\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">genero</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"81.6967\" cy=\"-105\" rx=\"27.8951\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"81.6967\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">baile</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M100.1093,-177.5504C93.2523,-171.7359 86.4749,-164.4043 82.6967,-156 79.5511,-149.0031 78.4199,-140.902 78.2998,-133.2449\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"81.7984,-133.3463 78.6688,-123.2243 74.8031,-133.0887 81.7984,-133.3463\"/>\n",
       "<text text-anchor=\"middle\" x=\"111.6967\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Masculino</text>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"162.6967\" cy=\"-105\" rx=\"32.4942\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.6967\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">vodka</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;D -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M131.2523,-174.5618C134.3633,-168.7118 137.7675,-162.1194 140.6967,-156 144.3851,-148.2943 148.1555,-139.8141 151.5093,-132.0183\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"154.8132,-133.1921 155.4971,-122.6194 148.3692,-130.458 154.8132,-133.1921\"/>\n",
       "<text text-anchor=\"middle\" x=\"178.1967\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\"> &#160;Femenino</text>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"37.6967\" cy=\"-18\" rx=\"37.8943\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"37.6967\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cerveza</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M73.0037,-87.8116C66.8176,-75.5801 58.3831,-58.9027 51.3092,-44.9158\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.3101,-43.0941 46.6737,-35.75 48.0635,-46.2533 54.3101,-43.0941\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.6967\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Si</text>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"125.6967\" cy=\"-18\" rx=\"32.4942\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.6967\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">vodka</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;E -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>B&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M90.3896,-87.8116C96.6164,-75.4996 105.1213,-58.6831 112.2235,-44.64\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"115.4812,-45.9538 116.8711,-35.4505 109.2347,-42.7945 115.4812,-45.9538\"/>\n",
       "<text text-anchor=\"middle\" x=\"114.1967\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">No</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fa8f0358450>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "dot = Digraph(comment='Recomendación contactologo')\n",
    "\n",
    "dot.node('A','genero')\n",
    "dot.node('B','baile')\n",
    "dot.node('C','cerveza')\n",
    "dot.node('D','vodka')\n",
    "dot.node('E','vodka')\n",
    "\n",
    "dot.edge('A','D',label='  Femenino')\n",
    "dot.edge('A','B',label='Masculino')\n",
    "dot.edge('B','C',label='Si')\n",
    "dot.edge('B','E',label='No')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f)  Proponer una poda del árbol y justificarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Implementing an ID3 Decision Tree ( problem suggested by: Sebastian Raschka (sraschka@wisc.edu)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Splitting a node \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to implement a function that splits a dataset along a feature axis into sub-datasets. Since we are going to implement a decision tree that only supports categorical features (like ID3) for simplicity, you do not need to account for continuous feature variables. In other words, the splitting function only needs to support integer NumPy arrays.  \n",
    "\n",
    "To provide an intuitive example, suppose you are given the following NumPy array with four feature values, feature values 0-3:\n",
    "\n",
    "    np.array([0, 1, 2, 1, 0, 3, 1, 0, 1, 2])\n",
    "    \n",
    "The function you are going to implement should return a dictionary, where each dictionary key represents a unique value in the array, and the values are the indices in that array that map to the respective feature value. Hence, based on the feature array above, your `split` function should return the following dictionary:\n",
    "\n",
    "    {0: array([0, 4, 7]), \n",
    "     1: array([1, 3, 6, 8]), \n",
    "     2: array([2, 9]), \n",
    "     3: array([5])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: I recommend you to use `np.where` and `np.unique` functions to make the implementation easier. If you do not remember these functions from the \"computational foundations\" lectures, you can either look up those functions in the NumPy documentation online, or you can execute `np.where?` and `np.unique?` in a new code cell to get more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def split(array):\n",
    "    res = { classi: np.where(array==classi)[0] for classi in np.unique(array) }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More efficient implementation\n",
    "from collections import defaultdict\n",
    "def split1(array):\n",
    "    res = defaultdict(list)\n",
    "    for i in range(len(array)):\n",
    "        res[array[i]].append(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0]), 1: array([1]), 2: array([2])}\n",
      "{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
      "{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}\n",
      "defaultdict(<class 'list'>, {0: [0], 1: [1], 2: [2]})\n",
      "defaultdict(<class 'list'>, {1: [0, 2, 5], 0: [1, 3, 4, 6]})\n",
      "defaultdict(<class 'list'>, {1: [0, 5, 6], 0: [1, 4], 3: [2], 2: [3]})\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(split(np.array([0, 1, 2])))\n",
    "print(split(np.array([1, 0, 1, 0, 0, 1, 0])))\n",
    "print(split(np.array([1, 0, 3, 2, 0, 1, 1])))\n",
    "\n",
    "print(split1(np.array([0, 1, 2])))\n",
    "print(split1(np.array([1, 0, 1, 0, 0, 1, 0])))\n",
    "print(split1(np.array([1, 0, 3, 2, 0, 1, 1])))\n",
    "\n",
    "\n",
    "#{0: array([0]), 1: array([1]), 2: array([2])}\n",
    "#{0: array([1, 3, 4, 6]), 1: array([0, 2, 5])}\n",
    "#{0: array([1, 4]), 1: array([0, 5, 6]), 2: array([3]), 3: array([2])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2) Implement Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the splitting function, we are now have to implement a criterion function so that we can compare splits on different features, to decide which feature is the best feature to split for growing the decision tree. As discussed in class, our splitting criterion will be Information Gain. However, before we implement an Information Gain function, we need to implement a function that computes the entropy at each node, which we need to compute Information Gain.\n",
    "\n",
    "For your reference, we defined entropy (i.e., Shannon Entropy) as follows:\n",
    "\n",
    "$$H(p) = \\sum_i p_i \\log_2 (1/p_i) = - \\sum_i p_i \\log_2 (p_i)$$\n",
    "\n",
    "where you can think of $p_i$ as the proportion of examples with class label $i$ at a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3, 1: 4, 2: 2, 3: 1})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def entropy(array):\n",
    "    classes = Counter(array)\n",
    "    n = len(array)\n",
    "    ent = 0\n",
    "    for k,v in classes.items():\n",
    "        pi = v/n\n",
    "        ent += pi * log(1/pi,2)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `entropy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.4395\n",
      "0.0\n",
      "1.6577\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(round(entropy(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(entropy(np.array([1, 2])), 4))\n",
    "print(round(entropy(np.array([1, 1])), 4))\n",
    "print(round(entropy(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([0, 0, 0])), 4))\n",
    "print(round(entropy(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))\n",
    "\n",
    "#1.0\n",
    "#1.0\n",
    "#0.0\n",
    "#0.4395\n",
    "#0.0\n",
    "#1.6577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Implement Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working solution for the `entropy` function, the next step is to compute the Information Gain. For your reference, information gain is computed as\n",
    "\n",
    "$$GAIN(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in Values(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(x_array, y_array):\n",
    "    parent_entropy = entropy(x_array)\n",
    "\n",
    "    split_dict = split(y_array)\n",
    "    \n",
    "    for val in split_dict.values():\n",
    "        freq = val.size/x_array.size\n",
    "        child_entropy = entropy([x_array[i] for i in val])\n",
    "        parent_entropy -= child_entropy*freq\n",
    "        \n",
    "    return parent_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `information_gain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591\n",
      "0.2516\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x = np.array([0, 1, 0, 1, 0, 1])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))\n",
    "\n",
    "x = np.array([0, 0, 1, 1, 2, 2])\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "print(round(information_gain(x, y), 4))\n",
    "\n",
    "#0.4591\n",
    "#0.2516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Decision Tree Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have all the main components that we need for implementing the ID3 decision tree algorithm: a `split` function, an `entropy` function, and an `information_gain` function based on the `entropy` function. \n",
    "\n",
    "The next task is combine these functions to recursively split a dataset on its different features to construct a decision tree that separate the examples from different classes well. We will call this function `make_tree`. \n",
    "\n",
    "For simplicity, the decision tree returned by the `make_tree` function will be represented by a Python dictionary. To illustrate this, consider the following dataset:\n",
    "\n",
    "```\n",
    "Inputs:\n",
    " [[0 0]\n",
    " [0 1]\n",
    " [1 0]\n",
    " [1 1]\n",
    " [2 0]\n",
    " [2 1]]\n",
    "\n",
    "Labels:\n",
    " [0 1 0 1 1 1]\n",
    "```\n",
    " \n",
    "This is a dataset with 6 training examples and two features.  The decision tree in form of the Python dictionary should look like as follows:\n",
    "\n",
    "\n",
    "\n",
    "You should return a dictionary with the following form:\n",
    "\n",
    "```\n",
    "{'X_1 = 0': {'X_0 = 0': array([0]),\n",
    "             'X_0 = 1': array([0]),\n",
    "             'X_0 = 2': array([1])},\n",
    " 'X_1 = 1': array([1, 1, 1])}\n",
    " ```\n",
    " \n",
    "Let me further illustrate what the different parts of the dictionary mean. Here, the `'X_1'` in `'X_1 = 0'` refers feature 2 (the first column of the NumPy array; remember that Python starts the index at 0, in contrast to R). \n",
    "\n",
    "- 'X_1 = 0': For training examples stored in this node, the second feature has the value 0\n",
    "- 'X_1 = 1': For training examples stored in this node, the second feature has the value 1\n",
    "\n",
    "The \"array\" is a NumPy array that stores the class labels of the training examples at that node. In the case of `'X_1 = 0'` we actually store actually a sub-dictionary, because this node can be split further. If you have trouble understanding this dictionary representation, the following illustration might help:\n",
    "\n",
    "\n",
    "![](tree-viz-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(X, y):\n",
    "    \n",
    "    # Return array if node is empty or pure (1 example in leaf node)\n",
    "    if y.shape[0] == 1 or y.shape[0] == 0:\n",
    "        return y\n",
    "\n",
    "    # Compute information gain for each feature\n",
    "    nfeatures = X.shape[1]\n",
    "    nsamples = X.shape[0]\n",
    "    features_values = [np.array([X[i][feature] for i in range(nsamples)]) for feature in range(nfeatures)]\n",
    "    gains = np.array([information_gain(features_values[feature],y) for feature in range(nfeatures)])\n",
    "\n",
    "    # Early stopping if there is no information gain\n",
    "    if (gains <= 1e-05).all():\n",
    "        return y\n",
    "    \n",
    "    # Else, get best feature\n",
    "    best_feature = np.argmax(gains)\n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Use the `split` function to split on the best feature\n",
    "    subset_dict = split(X[:, best_feature])\n",
    "    # Note that each entry in the dictionary returned by \n",
    "    # split is an attribute_value:array_indices pair.\n",
    "    # here, we are going to iterate over these key-value\n",
    "    # pairs and select the respective examples for the\n",
    "    # new child nodes\n",
    "    \n",
    "    for feature_value, train_example_indices in subset_dict.items():\n",
    "        child_y_subset = np.array([y[i] for i in train_example_indices])\n",
    "        child_x_subset = np.array([X[i] for i in train_example_indices])\n",
    "\n",
    "        # Next, we are using \"recursion,\" that is, calling the same\n",
    "        # tree_split function on the child subset(s)\n",
    "        \n",
    "        results[\"X_%d = %d\" % (best_feature, feature_value)] = \\\n",
    "                make_tree(child_x_subset, child_y_subset)\n",
    "\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [2 1]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x1 = np.array([0, 0, 1, 1, 2, 2])\n",
    "x2 = np.array([0, 1, 0, 1, 0, 1])\n",
    "X = np.array([x1, x2]).T\n",
    "y = np.array([0, 1, 0, 1, 1, 1])\n",
    "\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))\n",
    "\n",
    "#Inputs:\n",
    "# [[0 0]\n",
    "# [0 1]\n",
    "# [1 0]\n",
    "# [1 1]\n",
    "# [2 0]\n",
    "# [2 1]]\n",
    "\n",
    "#Labels:\n",
    "# [0 1 0 1 1 1]\n",
    "\n",
    "#Decision tree:\n",
    "#res =  {'X_1 = 0': {'X_0 = 0': array([0]), 'X_0 = 1': array([0]), 'X_0 = 2': array([1])}, 'X_1 = 1': array([1, 1, 1])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Building a Decision Tree API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of this part of the homework is now to write an API around our decision tree code so that we can use is for making predictions. Here, we will use the common convention, established by scikit-learn, to implement the decision tree as a Python class with \n",
    "\n",
    "- a `fit` method that learns the decision tree model from a training set via the `make_tree` function we already implemented;\n",
    "- a `predict` method to predict the class labels of training examples or any unseen data points.\n",
    "\n",
    "For making predictions, since not all leaf nodes are guaranteed to be single training examples, we will use a majority voting function to predict the class label as discussed in class. I already implemented a `_traverse` method, which will recursively traverse a decision tree dictionary that is produced by the `make_tree` function.\n",
    "\n",
    "Note that for simplicity, the `predict` method will only be able to accept one data point at a time (instead of a collection of data points). Hence `x` is a vector of size $\\mathbb{R}^m$, where $m$ is the number of features. I use capital letters `X` to denote a matrix of size $\\mathbb{R}^{n\\times m}$, where $n$ is the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3DecisionTreeClassifer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__fit = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.__fit = make_tree(X,y)\n",
    "        \n",
    "    def _majority_vote(self, label_array):\n",
    "        counter = Counter(label_array)\n",
    "        max_vote = max(counter.values())\n",
    "        counter = {v:k for k,v in counter.items()}\n",
    "        return counter[max_vote]\n",
    "        \n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            name, value = key.split(' = ')\n",
    "            feature_idx = int(name.split('_')[-1])\n",
    "            value = int(value)\n",
    "            if x[feature_idx] == value:\n",
    "                return self._traverse(x, d[key])\n",
    "        \n",
    "    def predict(self, x):        \n",
    "        label_array = self._traverse(x,self.__fit)\n",
    "        return self._majority_vote(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "tree = ID3DecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict(np.array([0, 0])))\n",
    "print(tree.predict(np.array([0, 1])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 0])))\n",
    "print(tree.predict(np.array([1, 1])))\n",
    "print(tree.predict(np.array([2, 0])))\n",
    "print(tree.predict(np.array([2, 1])))\n",
    "\n",
    "#0\n",
    "#1\n",
    "#0\n",
    "#0\n",
    "#1\n",
    "#1\n",
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
